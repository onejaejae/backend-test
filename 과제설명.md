# 과제 - Reqeust 가공 (중계서버)

## 문제해결

중계서버에서 Rate Limit없이 들어오는 요청들을 A서버의 RateLimit에 걸리지 않게 가공하기 위해, 우선 `userId` 별로 1분간 몇 번의 요청을 보냈는 지 기록해야겠다는 생각을 하였습니다.

### 📌 `userId` 별로 1분간 몇 번의 요청을 보냈는 지

해당 기능을 구현하기 위해서, 크게 2가지 방안을 생각하였습니다

1. 데이터베이스 ex. mongo, mysql ...

2. redis

<br>

위 2가지 방안 중 redis를 사용해 해당 기능을 구현하기로 결정하였습니다.
이유는 Redis는 in-memory key-value 데이터베이스로서, 기존의 ssd, hdd에 데이터를 저장하는 데이터베이스와 비교하여 데이터를 메모리에 저장하므로 빠른 응답 시간을 제공하기 때문입니다.

## redis를 사용 한 문제해결 개요

- redis `INCR` 명령어를 사용해, userId 마다 count를 합니다.

  - INCR 명령어는 시간 복잡도가 O(1)이기 때문에 성능이 빠릅니다.
  - INCR 명령어는 atomic 합니다.

- redis `EXPIRE` 명령어를 사용해 1초 만료시간을 설정해 키가 자동으로 삭제되도록 합니다.

<br>

즉, redis를 사용해 userId 마다 count를 하고, 1분동안 count의 합이 10 이상이라면, 해당 키가 만료될때까지 기라린 뒤, 다시 increment 하는 flow로 구현하였습니다.

<br>

## Trouble Shooting

### 코드 예시

```
 async proxy(userId: string) {
    const isRateLimitExceeded =
      await this.redisService.isRateLimitExceeded(userId);

    if (isRateLimitExceeded) {
      while (await this.redisService.isRateLimitExceeded(userId)) {
        await sleep(500);
      }

      await this.redisService.increment(userId);
      // A 서버 호출 로직
    } else {
      await this.redisService.increment(userId);
      // A 서버 호출 로직
    }
    return true;
  }
```

처음에는 위와 같이 코드를 구현하였습니다.<br>
코드를 구현한 뒤, artillery를 사용해 부하 테스트를 진행했을 때,
key 값이 userId에 대한 value 값이 10을 초과한 값이 발생하였습니다.<Br>
해당 문제가 발생하는 이유에 대해 고민해보았을 때, isRateLimitExceeded 값을 조회하는 과정에서 동시성 문제가 발생할 수 있다는 것을 알게되었습니다.

<Br>

### 코드 예시

```
 const isRateLimitExceeded =
    await this.redisService.isRateLimitExceeded(userId);

  if (isRateLimitExceeded) {
    while (await this.redisService.isRateLimitExceeded(userId)) {
      await sleep(500);
    }
```

<br>

### 참고 이미지

![redis](https://github.com/onejaejae/devops-practice/assets/62149784/008830fa-dc21-4ea0-9bec-9ad07bf609d9)

위 그림과 같이 여러 요청이 동시에 발생해 isRateLimitExceeded method를 호출한다면, `스레드 세이프하지 않은 상황이 발생할 수 있었습니다.`

### 문제 해결

`레디스 분산 락`을 통해 여러 노드가 동일한 자원에 접근하려 할 때, 일관성과 순서를 유지하면서 동시성 문제를 해결하였습니다.

<br>

## 단일 인스턴스로 구성된 레디스에서의 분산 락 구현

### flow

- 락을 획득할 때 키를 생성합니다. 만약 키가 이미 존재한다면 다른 클라이언트가 이미 락을 획득한 것이므로 키가 삭제될 때까지 기다립니다.

- 락을 반납하면서 키를 삭제합니다. 키가 삭제되면 기다리고 있던 다른 클라이언트 중 하나가 락을 획득하게 됩니다.

### 구현

#### 코드 예시

```
export class RedisDLM {
  constructor(@Inject('REDIS_CLIENT') private readonly redis: MyRedis) {
    redis.defineCommand('releaseLock', {
      numberOfKeys: 1,
      lua: `
  if redis.call("get", KEYS[1]) == ARGV[1] then
  return redis.call("del", KEYS[1])
  else
  return 0
  end
  `,
    });
  }

  // ...

  private async tryToAcquireLock(key: string) {
    const identity = uuidV4();
    const result = await this.redis.set(key, identity, 'PX', 30000, 'NX');

    return {
      success: result === 'OK',
      identity,
    };
  }
}
```

- NX 옵션을 사용해 키가 존재하지 않는 경우에만 키를 생성하였습니다. (Safety Property 보장)

- PX 옵션을 사용해 ms 단위로 키의 만료 시간을 설정하였습니다. (Liveness Property A 보장)

- uuid 라이브러리를 사용해 랜덤 값을 사용하였습니다.
  - 랜덤 값을 사용하는 이유는 락을 획득한 클라이언트만이 락을 반납할 수 있도록 구현하기 위해서입니다.
  - Lua 스크립트를 사용해 키가 존재하고 내가 생성했던 키라면 키를 삭제한다는 동작을 원자적으로 수행할 수 있도록 구현하였습니다.

#### 결과물

```
  async proxy(userId: string) {
    const lockKey = userId + 'lock';
    const identity = await this.redisDLM.acquireLock(lockKey);

    const isRateLimitExceeded =
      await this.redisService.isRateLimitExceeded(userId);

    if (isRateLimitExceeded) {
      while (await this.redisService.isRateLimitExceeded(userId)) {
        await sleep(500);
      }

      await this.redisService.increment(userId);
      // A 서버 호출 로직
    } else {
      await this.redisService.increment(userId);
      // A 서버 호출 로직
    }

    await this.redisDLM.releaseLock(lockKey, identity);
    return true;
  }
```

## 한계점

- 현재의 분산 락 구조는 Liveness Property B는 보장할 수 없는 한계점이 있습니다. 단일 인스턴스로 구성되어 있어 인스턴스 하나에서 장애가 발생하면 레디스 전체에 장애가 발생하게 되기 때문입니다.

### slave를 추가하면, 위 문제를 해결할 수 있을까?

해당 구조로는 slave를 추가하더라도 문제가 발생할 수 있다고 판단하였습니다.

![redis-failover](https://github.com/onejaejae/Slack/assets/62149784/b7015276-b952-437d-89d1-3fcfbc79d370)

위 그림과 같이, 모종의 이유로 Master Node Failover가 발생했을 때 다음과 같은 문제가 발생할 수 있습니다.

- Client A가 Master로부터 락을 획득합니다.
- Master에 키가 생성됩니다.
- Master에서 생성된 키가 Slave에 복제되기 전에 Master에 장애가 발생합니다.
- Slave는 Master로 승격됩니다.
- Client B가 락을 획득하려고 합니다.
- Master가 된 이전 Slave 노드에 키가 존재하지 않으므로 Client B는 락을 획득하고 Slave 노드는 키를 생성합니다.

즉, Client A와 Client B 모두 락을 획득한 상태가 됩니다.
즉, 여러 인스턴스로 구성된 레디스에 대해서 Safety Property의 상호 배제성을 보장할 수 없는 한계점이 있습니다.

### 현재 구조에서 redis의 부하가 가지 않을까?

현재 구조에서는 lock을 대기 중인 요청이 lock을 획득하기 위해 일정 주기로 lock을 획득할 때까지 redis에 요청을 보내고 있습니다.
이는 트래픽이 많아질 경우, redis의 많은 부하가 발생해 예기치 못한 상황이 발생할 수 있을꺼라는 생각이 들었습니다.

이러한 문제를 해결하는 방법이 무엇일까 검색하던 중, 자바 진영의 Redisson을 보며 인사이트를 얻었습니다. redisson은 스핀락으로 락을 점유하지 않고 pub/sub 구조로 레디스에 부하를 줄이는 구조를 채택하고 있다고 하였습니다.

만약, redis의 부하가 우려되는 상황이라면, pub/sub 구조로 개선하는 것을 고려해볼 수 있을꺼 같습니다.

### 또 다른 해결방안

메시지 큐를 사용하는 방법도 생각해보았습니다.
중계 서버에서 A 서버로의 요청을 Rate Limit 없이 메시지 큐에 넣고 Consumer는 메시지 큐에서 요청을 가져와 A 서버의 Rate Limit 규칙에 따라 처리할 수 있을 꺼 같습니다.

하지만, 실제로 메시지 큐를 사용해 구현해본 적은 없고 코드 레벨에서 디테일적으로 어떻게 구현할 지에 대한 러닝커브가 존재하였기에 위 방법을
적용하지는 못하였습니다.

<br>

# 코딩테스트 1,2

코딩테스트 1,2번 문제 모두 Key - Value 쌍의 데이터를 저장하고 조회하는 경우이기 때문에 Map을 사용하여 문제를 해결하였습니다.

왜냐하면 Map은 내부적으로 해시 테이블을 사용하여 Key - Value 쌍을 저장하므로 Map에서 키를 검색하는 경우, 평균적으로 O(1)의 시간복잡도를 가지기 떄문입니다.

<br>

# 테스트 코드

proxy 기능에 대한 테스트 코드를 작성해보았습니다.
